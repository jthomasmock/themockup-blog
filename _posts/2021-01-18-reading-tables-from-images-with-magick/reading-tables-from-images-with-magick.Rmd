---
title: "Reading tables from images with magick"
description: |
  magick is an R package for manipulating images in R
author:
  - name: Thomas Mock
    url: https://twitter.com/thomas_mock
date: 01-18-2021
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    self_contained: false
    highlight: default
    highlight_downlit: true
preview: norm_normal_file_format.png
twitter:
  site: "@thomas_mock"
  creator: "@thomas_mock"
categories:
  - magick
  - web scraping
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magick)
library(tesseract)
```

# ImageMagick

[ImageMagick](https://imagemagick.org/index.php) is a robust and comprehensive open-source image processing library, and per the official [docs](https://imagemagick.org/index.php):

> Use ImageMagick® to create, edit, compose, or convert bitmap images. It can read and write images in a variety of formats (over 200) including PNG, JPEG, GIF, HEIC, TIFF, DPX, EXR, WebP, Postscript, PDF, and SVG. ImageMagick can resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and Bézier curves.

While you can use it from various APIs, tools or CLIs, one of the easiest ways for R users to get started is with the R wrapper by ROpenSci's Jeroen Ooms called [`magick`](https://docs.ropensci.org/magick/index.html). This package provides a large set of pipe-friendly functions allowing for interactive editing and testing.

I've written briefly about `magick` before, specifically in using it to [add logos to final `ggplot2` images](https://themockup.blog/posts/2019-01-09-add-a-logo-to-your-plot/), but today will be a different use-case, namely using `magick` to read data embedded in images.

Another note is that while the docs for `ImageMagick` proper and the `magick` R wrapper are very good, `ImageMagick` is an entire piece of software. This means that there is an amazing breadth of applications, knowledge, and tricks to apply. I think of it a lot like `regex`, where it's very useful but for many applications we only scratch the surface. For a nice "cookbook" for using `ImageMagick`, check out this [resource](great resource: https://legacy.imagemagick.org/Usage/). It's a "legacy" guide, but many of the examples can be converted to `magick` in R or from the CLI itself.

# The problem

There are many times where someone shares data as an image, whether intentionally due to software constraints (ie Twitter) or as a result of not understanding the implications (image inside a PDF or in a Word Doc). [xkcd.com](https://xkcd.com/2116/) jokingly refers to this as `.norm` or as the Normal File Format. While it's far from ideal or a _real_ file format, it's all too common to see data as images in the "wild". 
I'll be using some examples from Twitter images and extracting the raw data from these. There are multiple levels of difficulty, namely that screenshots on Twitter are not uniform, often of relatively low quality (ie DPI), and contain additional "decoration" like colors or grid-lines. We'll do our best to make it work!


<aside>
```{r, echo = FALSE, fig.cap="https://xkcd.com/2116/"}

knitr::include_graphics("norm_normal_file_format.png")
```

</aside>


Example one is from Seth Walder, who was kind enough to share the raw data for ESPN's "Sacks created" stat. 
Given that it was a quality image, and there were 3 of them, I wanted to try OCR (optical character recognition)!

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">OK here&#39;s the top 46 players -- everyone with at least 5.5 sacks created.<br><br>Sacks created is an ESPN stat using NFL Next Gen Stats data. <a href="https://t.co/cNL23Dna9h">pic.twitter.com/cNL23Dna9h</a></p>&mdash; Seth Walder (@SethWalder) <a href="https://twitter.com/SethWalder/status/1349785021922631682?ref_src=twsrc%5Etfw">January 14, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

# Excel Screenshot

So let's get after it! We'll need two packages, but I'll also load `tidyverse` for the eventual analysis/plotting as well. Also, note that I adapted this from a [StackOverflow post](code adapted from: https://stackoverflow.com/questions/54000691/extracting-tables-from-jpeg-into-a-dataframe-in-r) talking about extracting tables from images.

```{r}
library(tidyverse)
library(magick)
library(tesseract)

sack_url <- "https://pbs.twimg.com/media/ErtmQ1PXYAEUQoY?format=jpg&name=900x900"
raw_img <- image_read(sack_url)
```

Now, while interactively you can just "print" the `magick` object and it will show up in the viewer pane, for this blog-post I'll need to explicitly "show" it. I'll call `magick::image_ggplot(<img object>)` to print it throughout the post.

```{r}
image_ggplot(raw_img)
```

This looks like a relatively clean image, the only major problems being that it's relatively low-DPI, it has some symbols (team logos), and it has alternating colors along with gridlines (which can mess with the OCR). We can try a "naive" OCR as seen below. 

```{r}
raw_img %>% 
  image_crop(geometry_area(0, 0, 110, 45)) %>% 
  ocr()
```

Ultimately, it does pretty well! But you can see that there are some "misses" if we look closely. Let's make it easier on the OCR engine by cleaning up the image for higher contrast.


## Remove the background and grid

We'll first start by converting the color image to greyscale via `image_quantize()`.

```{r}
raw_img %>% 
  image_quantize(colorspace = "gray") %>% 
  image_ggplot()
```

We can remove the grid by converting "white" colors to transparent, and allowing for some "fuzzy" approximation of colors that are close to white or "touching". There's a lot more to `fuzz` in that it's the "relative color distance (value between 0 and 100) to be considered similar in the filling algorithm", but I'm not a color space expert by any means.

Below we have an example of `fuzz = 0`, `fuzz = 20`, `fuzz = 40`, `fuzz = 60`. Each increase does remove a bit of "noise", but is also reducing the quality of the "signal".

<details><summary>Code for Combo</summary>
```{r}
fuzz_fun <- function(fuzz){
  raw_img %>% 
    image_quantize(colorspace = "gray") %>% 
    image_transparent(color = "white", fuzz=fuzz) %>% 
    image_background("white") %>% 
    image_crop(geometry_area(0, 150,110, 45))
}

fuzz_fun(20)

combo_fuzz <- c(
  fuzz_fun(0),
  fuzz_fun(20),
  fuzz_fun(40),
  fuzz_fun(60)
) %>% 
  image_append(stack = TRUE) 
```

</details>

```{r}
image_ggplot(combo_fuzz)
```

In practical terms, we are balancing increase `fuzz` to remove unnecessary components (eg grid lines) while leaving the actual characters there. Increasing fuzz will remove more "noise" but will eventually start to eat away at the actual "signal" as well.

```{r}
no_grid <- raw_img %>% 
  image_quantize(colorspace = "gray") %>% 
   image_transparent(color = "white", fuzz=20) %>% 
   image_background("white") 

image_ggplot(no_grid)
```

So we've taken white and converted it to transparent, and then set the image background back to "white".

You can also remove continuous lines with a "thinning" method. We can use `image_morphology()` coupled with a rectangular kernel to remove straight horizontal lines for the most part. You can read `Rectangle:20x1` as finding rectangles about 20 pixels wide x 1 pixel high. We couple this with `image_negate()` as otherwise it will focus on the characters.

So we'll `negate` > `thin` > `negate` to get back to our white background sans grid-lines. While this works pretty well, it's not _always_ necessary for the OCR. I did want to show it as it can be helpful in some situations.

```{r}
no_grid %>% 
  image_negate() %>% 
  image_ggplot()
```

```{r}
no_grid %>%
  image_negate() %>% # negate
  image_morphology(method = "Thinning", kernel = "Rectangle:20x1") %>%
  image_negate() %>% # back to white
  image_ggplot()
```

This worked pretty well! However, remember that we don't always have to do this. We'll also still need to crop the image to remove the team logos as they can't be parsed as text.

## Crop the image

`geometry_area()` is used in various functions to indicate the starting width/heights and then the offset, all in pixels.  

> `geometry_area(width = NULL, height = NULL, x_off = 0, y_off = 0)`  

Note that you're always "starting" from the top and left sides, and we're passing the `geometry_area()` to `image_crop` to crop the image itself.

```{r}
# remove the top 20 pixels
no_grid %>% 
  image_crop(geometry_area(0, 0,110, 45)) %>% 
  image_ggplot()
```

So this techniques can be used to cut out specific portions of an image, which is another useful technique for tricky columns. For now, let's hope we can use ALL the data together.

```{r}
no_grid_crop <- no_grid %>% 
  image_crop(geometry_area(0, 0,110, 45))

no_grid_crop %>% 
  image_ggplot()
```

## Try OCR

We can try our first OCR now! Note that `image_ocr()` is just calling `tesseract` behind the scenes.

```{r}
no_grid_crop %>% 
  image_ocr()
```

This did a great job, but what about this raw text string we ended up with? Also note that `image_ocr()` is just a wrapper around `tesseract::ocr()`.

Let's go into `tesseract` proper to try some robust things out!

I'm going to focus on one numeric "column" first to keep things relatively simpler. We'll use `image_crop` to grab the column of interest, then we'll call `tesseract::ocr()` on it. We can provide some options to the engine, namely that we're expecting only spaces, numbers, or a decimal. This will explicitly prevent 5 being converted to S for example. It does really well here!

```{r}
num_only <- tesseract::tesseract(
  options = list(tessedit_char_whitelist = c(".0123456789 "))
  )

no_grid %>% 
  image_quantize(colorspace = 'gray') %>% 
  image_threshold() %>% 
  image_crop(geometry_area(100, 0, 600, 40)) %>% 
  ocr(engine = num_only) 
```

But we have text, numbers, and some symbols like `(`.

So let's pass those as limitations to the engine, and then we can take the raw text and turn it into a dataframe/tibble.

```{r}
combo <- tesseract::tesseract(
    options = list(
      tessedit_char_whitelist = paste0(
        c(letters, LETTERS, " ", ".0123456789 (-)"), collapse = "")
      )
  )

raw_text <- no_grid %>%
  image_quantize(colorspace = "gray") %>%
  image_transparent("white", fuzz = 22) %>%
  image_background("white") %>%
  image_threshold() %>%
  image_crop(geometry_area(0, 0, 110, 45)) %>%  
  ocr(engine = combo)
```

## Make a Tibble

```{r}
raw_text
```

This looks pretty much perfect! We just now need to get it into a dataframe. We can accomplish this by splitting on each new row (`\n`) and then adding that as a column in a tibble.

```{r}
raw_tibble <- raw_text %>% 
  str_split(pattern = "\n") %>% 
  unlist() %>%
  tibble(data = .) 

raw_tibble
```

Now we have essentially perfect data at this point, we just need to separate out our columns and drop the row without any data.

## Tidy the Tibble

We'll first drop any rows where the character string is < 2. We can then use `tidyr::separate()` to separate one column into 4 new columns as player, position, team, and sacks. There are multiple separators, which for the example seen `T.J. Watt (OLB - PIT) 14.5` are:  

- `T.J.Watt` then ` (` represented as "` \\(`"  
- `OLB` then ` - ` represented as "` - `"  
- `PIT` then `) ` represented as "`\\) `"  
- which leaves us with `14.5` at the end  

We have to "escape" the parentheses with `\\` so that regex can understand them. The `|` inside the sep arguments tell the regex to separate at a match of a white-space + parentheses OR white-space + dash OR parentheses + white-space.

As our last step, we'll convert sacks to a double column, and then **BOOM**!

```{r}
raw_tibble %>% 
  filter(str_length(data) >= 2)  %>%
  separate(
    data, 
    into = c("player", "position", "team", "sacks"), 
    sep = c(" \\(| - |\\) ")
    ) %>% 
  mutate(sacks = as.double(sacks))
```

## Write it as a Function

We can wrap this process into a reusable function, and then call it on our 3 images of interest!

```{r}
scrape_fun <- function(url_in, crop_left, crop_top){
  raw_img <- image_read(url_in) %>% 
    image_quantize(colorspace = 'gray') %>%
    image_transparent("white", fuzz=22) %>% 
    image_background("white") %>%
    image_threshold() %>% 
    image_crop(geometry_area(0, 0, crop_left, crop_top)) 
  
  image_ocr(raw_img) %>% 
    str_c() %>% 
    str_split(pattern = "\n") %>% 
    unlist() %>%
    tibble(data = .) %>% 
    filter(str_length(data) >= 2) %>% 
    separate(
      data, 
      into = c("player", "position", "team", "sacks"), 
      sep = c(" \\(| - |\\) ")
      ) %>% 
    mutate(sacks = as.double(sacks)) %>% 
    mutate(sacks = if_else(sacks >= 20, sacks/10, sacks))
}
```


We can then call our new function! You may notice that I added a `mutate` above which protects against missing decimal places. Trey Hendrickson's decimal place is apparently hard for the parser to "capture".

```{r}
# output to tibble
cr_sacks <- tibble(
  url_in = c(
    "https://pbs.twimg.com/media/ErtmQ1PXYAEUQoY?format=jpg&name=900x900",
    "https://pbs.twimg.com/media/ErtmSLlXMAAvylA?format=jpg&name=900x900",
    "https://pbs.twimg.com/media/ErtmTUGW8AEZ6Cy?format=jpg&name=900x900"
    ),
  crop_left = c(110, 95, 95),
  crop_top = c(45, 5, 5)
) %>% 
  pmap_df(scrape_fun)

cr_sacks
```

We can plot it just to be safe, and to see if everything "checks out"! We get a range we expected, and see that T.J. Watt and Aaron Donald create many more of their own sacks relative to their positions.

```{r, fig.width=10, fig.height = 6, dpi = 150, layout="l-body-outset"}
cr_sacks %>% 
  mutate(position = if_else(position == "LB", "OLB", position)) %>% 
  ggplot(aes(x = sacks, y = position)) +
  ggridges::geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  geom_point(
    data = filter(cr_sacks, player %in% c("Aaron Donald", "T.J. Watt")),
    size = 3
    ) +
  ggridges::theme_ridges() +
  theme(
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 14),
    plot.title = element_text(size = 16),
    plot.caption = element_text(size = 10),
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(hjust = 0)
    ) +
  labs(
    x = "\nCreated Sacks", y = "",
    title = "T.J. Watt and A. Donald are both outliers amongst their positions",
    subtitle = "Created Sacks by position",
    caption = "Data: ESPN | Plot: @thomas_mock"
    )
```


# Example 2

Example two is from Brian Burke, who was kind enough to share the raw data for NFL playoff leverage for week 17 as a screenshot. At the time, I manually copied these over to a dataframe, but Ben Baldwin asked if there was a "better" way. Image OCR (optical character recognition) is a potentially more reproducible way!

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Sorry, no pretty charts for now. But here are the raw numbers. <a href="https://t.co/wZ3j9bXzTN">pic.twitter.com/wZ3j9bXzTN</a></p>&mdash; Brian Burke (@bburkeESPN) <a href="https://twitter.com/bburkeESPN/status/1339616433408528384?ref_src=twsrc%5Etfw">December 17, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Note that our process is almost identical to the previous example, but there are some considerations as the image is a bit more difficult to work with.

Now, this image is more problematic for two main reasons.  

1. The image itself is _much_ smaller, specifically about 60% smaller than our previous example and of low quality  
2. The fonts in use are worse (not monospaced and have serifs), and not bold  

This is going to be **harder** than before.

## Get the raw img

```{r}
burke <- "https://pbs.twimg.com/media/EpdF4fzW4AEeOvF?format=png&name=small"
raw_img <- image_read(burke)
```

Now, again while interactively you can just "print" the `magick` object and it will show up in the RStudio viewer pane, for this blog-post I'll need to explicitly "show" it. I'll call `magick::image_ggplot(<img object>)` to print it throughout the post.

```{r}
image_ggplot(raw_img)
```

This looks like a relatively clean image, the only major problems as mentioned above are that it's very low-quality along with not ideal fonts, and it cuts off some of the data for team.

## Clean it up

We can remove the grid by converting "white" colors to transparent, and allowing for some "fuzzy" approximation of colors that are close to white or "touching". There's a lot more to `fuzz` in that it's the "relative color distance (value between 0 and 100) to be considered similar in the filling algorithm", but I'm not a color space expert by any means.

In practical terms, we are balancing increase `fuzz` to remove unnecessary components (eg grid lines) while leaving the actual characters there. Increasing fuzz will remove more "noise" but will eventually start to eat away at the actual "signal" as well.

Below we have an example of `fuzz = 0`, `fuzz = 20`, `fuzz = 50`, `fuzz = 70`.

So we've taken white and converted it to transparent, and then set the image background back to "white".


```{r}
no_grid <- raw_img %>% 
   image_transparent(color = "white", fuzz=20) %>% 
   image_background("white") 

image_ggplot(no_grid)
```

You can also remove continuous lines with a "thinning" method. We can use `image_morphology()` coupled with a rectangular kernel to remove straight horizontal lines for the most part. You can read `Rectangle:20x1` as finding rectangles about 20 pixels wide x 1 pixel high. We couple this with `image_negate()` as otherwise it will focus on the characters.

So we'll `negate` > `thin` > `negate` to get back to our white background sans grid-lines.

```{r}
no_grid %>% 
  image_negate() %>% 
  image_ggplot()
```


We can apply the thinning morph as seen below, flipping back and forth with `image_negate()`.

```{r}
no_grid %>% 
  image_negate() %>%
  image_morphology(method = "Thinning", kernel = "Rectangle:20x1") %>% 
  image_negate() %>% 
  image_ggplot()
```

This worked pretty well! However, because of the mis-alignment of the column labels and the columns themselves, I'm just going to trim the top off. 

## Crop the image

`geometry_area()` is used in various functions to indicate the starting width/heights and then the offset, all in pixels.  

> `geometry_area(width = NULL, height = NULL, x_off = 0, y_off = 0)`  

Note that you're always "starting" from the top and left sides, and we're passing the `geometry_area()` to `image_crop` to crop the image itself.

```{r}
# remove the top 20 pixels
no_grid %>% 
  image_crop(geometry_area(0, 0, 0, 20)) %>% 
  image_ggplot()
```

So this techniques can be used to cut out specific portions of an image, which is another useful technique for tricky columns. For now, let's hope we can use ALL the data together.

```{r}
no_grid_crop <- no_grid %>% 
  image_crop(geometry_area(0, 0, 0, 20))

no_grid_crop %>% 
  image_ggplot()
```

## Try OCR

We can try our first OCR now! Note that `image_ocr()` is just calling `tesseract` behind the scenes.

```{r}
no_grid_crop %>% 
  image_ocr()
```

This did a decent job, but I can already see some "problems".

The Rams row is "squished" together, the Browns have some letters instead of numbers, and the Ravens have a "S" instead of a 5.

Let's go into `tesseract` proper to try some robust things out!

I'm going to focus on one "column" first to keep things relatively simpler. We'll use `image_crop` to grab the column of interest, then we'll call `tesseract::ocr()` on it. We can provide some options to the engine, namely that we're expecting only spaces, numbers, or a decimal. This will explicitly prevent 5 being converted to S for example.

```{r}
num_only <- tesseract::tesseract(
  options = list(tessedit_char_whitelist = c(".0123456789 "))
  )
no_grid %>% 
  image_quantize(colorspace = 'gray') %>% 
  image_threshold() %>% 
  image_crop(geometry_area(80, 0, 80, 20)) %>% 
  ocr(engine = num_only) 
```

I'm going to "plot" the data to show some areas where mistakes were _still_ made. We know that 100 is the max, and that the values are ranked, and thus should be always decreasing. We have at least 5 examples where the data is missing a period which causes it to be scaled improperly (10x larger than reality).

```{r}
ocr_col1 <- no_grid %>%
  image_crop(geometry_area(80, 0, 80, 20)) %>%
  ocr(engine = num_only) %>%
  str_split(pattern = "\n") %>%
  unlist() %>%
  enframe() %>%
  mutate(value = as.double(value)) %>%
  filter(!is.na(value))

ocr_col1 %>%
  mutate(color = case_when(
    value > 100 ~ "red",
    value > lag(value) ~ "red",
    value > lag(value, n = 3) ~ "red",
    TRUE ~ "black"
  )) %>%
  ggplot(aes(x = name, y = value, color = color)) +
  geom_point(size = 3) +
  scale_color_identity()
```

Again, due to the VERY low image quality (the original image is 72 DPI and only ~500 pixels high) we're basically stuck with some of these manual steps. Regardless, we've got "better" data now that we've added our conversion logic. Those same 5 points now fall "accurately" into the appropriate range.

```{r}
ocr_col1 %>% 
  mutate(color = case_when(
    value > 100 ~ "red",
    value > lag(value) ~ "red",
    value > lag(value, n = 3) ~ "red",
    TRUE ~ "black"
  )) %>% 
  mutate(
    value = if_else(value > 100, value/10, value),
    value = if_else(name >= 22, value/10, value)
    ) %>% 
  ggplot(aes(x = name, y = value, color = color)) +
  geom_point(size = 3) +
  scale_color_identity()
```

So we've apparently "fixed" this column!

## Create a function

We'll create a function so that we can split out each column, apply optimal characters or numeric to each and then recombine.

```{r}
img_ocr_fun <- function(trim_width, trim_start, char_num = TRUE) {
  
  num_only <- tesseract::tesseract(
    options = list(tessedit_char_whitelist = c(".0123456789 "))
  )

  combo <- tesseract::tesseract(
    options = list(
      tessedit_char_whitelist = paste0(
        c(letters, LETTERS, " ", ".0123456789 "), collapse = "")
      )
  )


  input_char <- if (isTRUE(char_num)) {
    num_only
  } else {
    combo
  }

  no_grid %>%
    image_crop(geometry_area(trim_width, 0, trim_start, 20)) %>%
    ocr(engine = input_char) %>%
    str_split(pattern = "\n") %>%
    unlist() %>%
    enframe() %>%
    select(-name) %>%
    filter(!is.na(value), str_length(value) > 0)
}
```

We can "find" the columns by cropping specific areas. Note that I've "recombined" all of them with `image_append()` so that you can see that each section together completes the table.

```{r}
c(
  no_grid %>%
    image_crop(geometry_area(80, 0, 0, 20)),
  no_grid %>%
    image_crop(geometry_area(50, 0, 80, 20)),
  no_grid %>%
    image_crop(geometry_area(50, 0, 140, 20)),
  no_grid %>%
    image_crop(geometry_area(50, 0, 210, 20))
) %>%
  image_append() %>%
  image_ggplot()
```

## Apply the function

We can use `purrr::pmap()` to apply the functions with each of our parameters, and then use `bind_cols` to create our actual dataset.

```{r}

all_ocr <- list(trim_width = c(80, 50, 50, 50),
     trim_start = c(0, 80, 140, 210),
     char_num = c(FALSE, TRUE, FALSE, TRUE)) %>% 
  pmap(img_ocr_fun) 

data_df <- all_ocr %>% 
  bind_cols() %>% 
  set_names(nm = "team", "win", "lose", "leverage") 

data_df
```

## Clean it up

We can convert some of the letters to their proper numeric. I intentionally used characters vs numeric on some columns as it "guesses" better in our really low quality image. I then add some logic to convert our numbers if decimal places are missing, and it gets VERY close, but is not perfect.

```{r}

data_df %>% 
  mutate(across(win:leverage, ~str_replace(tolower(.x), "s", "5"))) %>% 
  mutate(across(win:leverage, ~str_replace(tolower(.x), "o|a", "0"))) %>% 
  mutate(across(win:leverage, as.double)) %>% 
  mutate(across(win:leverage, ~if_else(.x > 100, .x/10, .x))) %>% 
  mutate(lose = if_else(lose > win, lose/10, lose)) %>% 
  mutate(leverage = win - lose) %>% 
  print(n = 25)
```

It looks really good, and it really only messed up a few rows, which we could fix manually, but you can see that while these techniques are robust you are still at the mercy of image quality!

